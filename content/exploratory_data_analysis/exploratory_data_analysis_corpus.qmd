---
title: "Exploratory Data Analysis"
author: "Keenan Smith"
editor_options: 
  chunk_output_type: console
---

Built with R Version `r getRversion()`

```{r}
#| output: false
#| label: Reading in initial Libraries
here::i_am("content/exploratory_data_analysis/exploratory_data_analysis_corpus.qmd")
library(tidytable)
library(ggplot2)
library(quanteda)
library(quanteda.textstats)
library(quanteda.textplots)
```

# ONLY RUN IF FIRST TIME

**Takes about 20 mins to get all data into R and tokenized.**

## Importing Corpus and Filtering

Eliminated Human Rights Watch since after initial inspection, the text is mainly about non-English subjects and represents a vast majority of the important Left Wing words. 

```{r}
require(DBI)
pol_parquet_db <- dbConnect(duckdb::duckdb(), dbdir = here::here("data","pol-parquet.duckdb"))

corpus_pull <- function(tbl_title, con) {
  filter_date <- lubridate::ymd("20100101")
  dplyr::tbl(con, tbl_title) |>
    dplyr::left_join(source_table, by = source_table$art_source) |>
    dplyr::filter(art_date >= filter_date) |>
    dplyr::collect() |>
    tidytable::as_tidytable() |>
    tidytable::mutate(
    short_source = as.factor(short_source),
    source_bias = as.factor(source_bias),
    length_text = nchar(full_text),
    total_words = tokenizers::count_words(full_text)) |>
    tidytable::rename(text = full_text)
}


source_table <- dplyr::tbl(pol_parquet_db, "source_table")

table_aei <- corpus_pull("text_aei", pol_parquet_db)
table_am <- corpus_pull("text_am", pol_parquet_db)
table_brook <- corpus_pull("text_brook", pol_parquet_db)
table_cap <- corpus_pull("text_cap", pol_parquet_db)
table_cato <- corpus_pull("text_cato", pol_parquet_db)
table_cbpp <- corpus_pull("text_cbpp", pol_parquet_db)
table_cfr <- corpus_pull("text_cfr", pol_parquet_db)
table_comf <- corpus_pull("text_comf", pol_parquet_db)
table_demos <- corpus_pull("text_demos", pol_parquet_db)
table_disc <- corpus_pull("text_disc", pol_parquet_db)
table_epi <- corpus_pull("text_epi", pol_parquet_db)
table_fab <- corpus_pull("text_fab", pol_parquet_db)
table_gutt <- corpus_pull("text_gutt", pol_parquet_db)
table_heritage <- corpus_pull("text_heritage", pol_parquet_db)
table_iiss <- corpus_pull("text_iiss", pol_parquet_db)
table_jacob <- corpus_pull("text_jacob", pol_parquet_db)
table_mani <- corpus_pull("text_mani", pol_parquet_db)
table_merc <- corpus_pull("text_merc", pol_parquet_db)
table_osf <- corpus_pull("text_osf", pol_parquet_db)
table_third <- corpus_pull("text_third", pol_parquet_db)
table_urban <- corpus_pull("text_urban", pol_parquet_db)
table_wilson <- corpus_pull("text_wilson", pol_parquet_db)

corpus_df <- bind_rows(table_aei, table_am, table_brook, table_cap, table_cato, table_cbpp,
                      table_cfr, table_comf, table_demos, table_disc, table_epi, table_fab,
                      table_gutt, table_heritage, table_iiss, table_jacob, table_mani, table_merc, table_osf,
                      table_third, table_urban, table_wilson)

bigrams_test <- corpus_df |>
  tidytable::mutate(
    bigrams = tokenizers::tokenize_ngrams(text, n = 2, ngram_delim = "_")
  )

readr::write_rds(bigrams_test, here::here("data", "tokens", "bigrams_test.rds"), compress = "gz")
# DBI::dbDisconnect(pol_parquet_db, shutdown = TRUE)
```

```{r}
corpus_length <- corpus_df |>
  select(art_link, art_date, art_source, short_source, source_bias, length_text)
fwrite(corpus_length, here::here("content", "project", "sample_data", "corpus_length.csv"))


summary(corpus_df)
```

```{r}
library(scales)

ggplot(corpus_df, aes(x = length_text, fill = short_source, color = short_source)) +
  geom_histogram(alpha = 0.5, position = "identity") +
  scale_x_log10(labels = label_number(scale_cut = cut_short_scale())) +
  labs(x = "Length Text",
       y = "Number of Characters (log scale)") +
  theme_classic()
ggsave(here::here("content", "project", "images", "corpus_histogram_source.png"))

ggplot(corpus_df, aes(x = short_source, y = length_text)) +
  geom_boxplot() +
  scale_y_log10(labels = label_number(scale_cut = cut_short_scale())) +
  labs(x = "Length Text",
       y = "Number of Characters (log scale)")
ggsave(here::here("content", "project", "images", "corpus_boxplot_source.png"))

ggplot(corpus_df, aes(x = length_text, fill = source_bias, color = source_bias)) +
  geom_histogram(alpha = 0.5, position = "dodge") +
  scale_x_log10(labels =label_number(scale_cut = cut_short_scale())) +
  scale_color_manual(values = c("left-wing" = "blue",
                                "right-wing" = "red")) +
  scale_fill_manual(values = c("left-wing" = "blue",
                                "right-wing" = "red")) +
  labs(x = "Length Text",
       y = "Number of Characters (log scale)")
ggsave(here::here("content", "project", "images", "corpus_histogram_bias.png"))

ggplot(corpus_df, aes(x = source_bias, y = length_text)) +
  geom_boxplot() +
  scale_y_log10(labels = label_number(scale_cut = cut_short_scale())) +
  labs(x = "Length Text",
       y = "Number of Characters (log scale)")

ggsave(here::here("content", "project", "images", "corpus_boxplot_bias.png"))
```

```{r}
group_source <- corpus_df |>
  group_by(short_source) |>
  count(sort = TRUE)

total_length <- sum(corpus_df$length_text)

# Summary Statistics for the Filtered Corpus
bias_count <- corpus_df |>
  group_by(source_bias) |>
  summarise(total = sum(length_text),
            num = n(),
            min = min(length_text),
            max = max(length_text),
            mean = mean(length_text),
            median = mean(length_text),
            iqr = IQR(length_text),
            sd = sd(length_text))
fwrite(bias_count, here::here("content", "project", "bias_count.csv"))

group_count <- corpus_df |>
  group_by(short_source) |>
  summarise(total = sum(length_text),
            num = n(),
            min = min(length_text),
            max = max(length_text), 
            mean = mean(length_text),
            median = mean(length_text),
            iqr = IQR(length_text),
            sd = sd(length_text))
fwrite(group_count, here::here("content", "project", "source_count.csv"))

# Trying to Get Roughly Equal Character Counts
bias_count$total[[1]] / total_length
bias_count$total[[2]] / total_length
```

```{r}
#| label: Adjusted Corpus

aei_90 <- quantile(table_aei$length_text, .90)
heritage_92 <- quantile(table_heritage$length_text, .92)

table_aei_adj <- table_aei |>
  filter(length_text < aei_90)

table_heritage_adj <- table_heritage |>
  filter(length_text < heritage_92)

corpus_df_filtered <- bind_rows(table_aei_adj, table_am, table_brook, table_cap, table_cato, table_cbpp,
                      table_cfr, table_comf, table_demos, table_disc, table_epi, table_fab,
                      table_gutt, table_heritage_adj, table_iiss, table_jacob, table_mani, table_merc, table_osf,
                      table_third, table_urban, table_wilson)

corpus_df_fil_5 <- quantile(corpus_df_filtered$length_text, .05)

corpus_df_filtered <- corpus_df_filtered |>
  filter(length_text > corpus_df_fil_5)

corpus_length_filtered <- corpus_df_filtered |>
  select(art_link, art_date, art_source, short_source, source_bias, length_text)
fwrite(corpus_length_filtered, here::here("content", "project", "sample_data", "corpus_length_filtered.csv"))
```

```{r}
total_length_filtered <- sum(corpus_df_filtered$length_text)

# Summary Statistics for the Filtered Corpus
bias_count_filtered <- corpus_df_filtered |>
  group_by(source_bias) |>
  summarise(total = sum(length_text),
            num = n(),
            min = min(length_text),
            max = max(length_text),
            mean = mean(length_text),
            median = mean(length_text),
            iqr = IQR(length_text),
            sd = sd(length_text))

fwrite(bias_count_filtered, here::here("content", "project", "bias_count_filtered.csv"))

group_count_filtered <- corpus_df_filtered |>
  group_by(short_source) |>
  summarise(total = sum(length_text),
            num = n(),
            min = min(length_text),
            max = max(length_text), 
            mean = mean(length_text),
            median = mean(length_text),
            iqr = IQR(length_text),
            sd = sd(length_text))
fwrite(group_count_filtered, here::here("content", "project", "source_count_filtered.csv"))

# Trying to Get Roughly Equal Character Counts
bias_count_filtered$total[[1]] / total_length_filtered
bias_count_filtered$total[[2]] / total_length_filtered
```

```{r}
ggplot(corpus_df_filtered, aes(x = length_text, fill = short_source, color = short_source)) +
  geom_histogram(alpha = 0.5, position = "identity") +
  scale_x_log10(labels = label_number(scale_cut = cut_short_scale())) +
  labs(x = "Length Text",
       y = "Number of Characters (log scale)") +
  theme_classic()
ggsave(here::here("content", "project", "images", "corpus_histogram_filtered_source.png"))

ggplot(corpus_df_filtered, aes(x = short_source, y = length_text)) +
  geom_boxplot() +
  scale_y_log10(labels = label_number(scale_cut = cut_short_scale())) +
  labs(x = "Length Text",
       y = "Number of Characters (log scale)")
ggsave(here::here("content", "project", "images", "corpus_boxplot_filtered_source.png"))

ggplot(corpus_df_filtered, aes(x = length_text, fill = source_bias, color = source_bias)) +
  geom_histogram(alpha = 0.5, position = "dodge") +
  scale_x_log10(labels =label_number(scale_cut = cut_short_scale())) +
  scale_color_manual(values = c("left-wing" = "blue",
                                "right-wing" = "red")) +
  scale_fill_manual(values = c("left-wing" = "blue",
                                "right-wing" = "red")) +
  labs(x = "Length Text",
       y = "Number of Characters (log scale)")
ggsave(here::here("content", "project", "images", "corpus_histogram_filtered_bias.png"))

ggplot(corpus_df_filtered, aes(x = source_bias, y = length_text)) +
  geom_boxplot() +
  scale_y_log10(labels = label_number(scale_cut = cut_short_scale())) +
  labs(x = "Length Text",
       y = "Number of Characters (log scale)")
ggsave(here::here("content", "project", "images", "corpus_boxplot_filtered_bias.png"))
```

# Creating Sample Data for Presentation

```{r}
sample_corpus <- corpus_df_filtered |>
  slice_sample(n = 5, .by = source_bias) |>
  select(art_link, art_date, text, source_bias)
fwrite(sample_corpus, here::here("content", "project", "sample_corpus.csv"))
```

## Creating Corpus Object

Added Unique Document Names

```{r}
#| label: Quanteda Operations
corpus_polsent <- corpus(corpus_df_filtered)

docid <- paste(corpus_df_filtered$short_source,
               corpus_df_filtered$pull_index,
               corpus_df_filtered$source_bias,
               sep = "_")

docnames(corpus_polsent) <- docid

readr::write_rds(corpus_polsent, here::here("data", "tokens", "corpus.rds"), compress = "gz")

# All stopwords were selected by Initial Analysis of the Total Dataset
# there may be more as the Corpus is quite large but these had the biggest effect on the Data
additional_stopwords <- c("epop", "percision", "embedclicker", "textarea", "getembedcode", "footerright", "footerleft", "customjson", "attr", "thead", "styledtable", "roboto", "tbody", "monospace", "scopedstyledtable", "xaxis", "plotbands", "plotoptions", "chartinfo", "yaxistitle", "yaxismin", "yaxismax", "yaxisvisibility", "xaxistitle", "xaxismultipletitles", "xaxisunits", "xaxisplotbands", "defaultoffset", "orderby", "showdatalabels", "showfirstdatalabel", "showlastdatalabel", "decimalplaces", "heightadjustment", "epicharts", "showscatterlabels", "showscattermarkers", "showregressionline", "showregressionequation", "regressionlabel", "regressionslope", "regressionintercept", "chartdatadownload", "verticalalign", "f", "embed", "layout", "download", "chart", "copy", "facebook", "y", "x", "figure", "data", "null", "stacking", "photo", "height", "semihidden", "quot", "background", "align", "backgrounder", "swf", "istockphoto", "jquery", "image", "tweet", "id", "title", "browser", "unhidewhenused", "scribd", "post", "told", "click", "ref", "said")
# Ensuring that Sources were removed from the Corpus
source_stopwords <- c("enterprise", "institute", "cato", "heritage", "foundation", "center", "mercatus", "manhattan", "cbpp", "discovery", "hoover", "claremont", "guttmacher", "jacobin", "commonwealth", "epi", "aei", "cap", "human", "watch", "hrw", "woodrow", "wilson", "third", "way", "demos", "fabian", "Berggruen", "society", "strategic", "studies", "council")

bigram_stopterms <- c("amp_amp", "percent_percent", "originally_appeared", "piece_originally", "cdata_cdata", "appeared_daily", "share_share", "right_top", "code_website", "share_twitter", "research_insights", "para_para", "n_n", "miss_research", "yes_yes", "enjoyed_sign", "excel_underlying", "legend_position", "type_legend", "visibility_type", "false_visibility", "enabled_true", "type_line", "miss_posts", "related_reading", "appeared_forbes", "posts_papers", "recently_highlights", "fellow_contributing", "papers_charts", "editor_city", "outlets_featured", "appeared_realclearmarkets", "fellow_follow", "grid_table", "name_grid", "code_var", "floating_false", "var_shared", "false_text", "hidden_enabled", "underlying_note", "line_false", "twitter_variety", "table_colorful", "top_calc", "tab_format", "bottom_horizontal", "display_tab", "bold_arial", "none_border", "none_linear", "buffon_rgba", "transparent_rgba", "color_transparent", "rgba_buffon", "table_accent")

bigrams_stopterms <- unique(unlist(strsplit(bigram_stopterms, "_")))

first_model_stopterms <- c("desmond", "lachman", "kathryn", "shelby", "cullom", "davis", "journal", "review", "online", "scholar", "examiner", "times", "george", "mason", "university", "email")

stopterms <- c(additional_stopwords, source_stopwords, bigrams_stopterms, first_model_stopterms)

# toks_polsent_unfiltered <- tokens(corpus_polsent, remove_punct = TRUE, remove_symbols = TRUE, remove_numbers = TRUE)

toks_polsent <- tokens(corpus_polsent, remove_punct = TRUE, remove_symbols = TRUE, remove_numbers = TRUE) |>
  tokens_remove(pattern = c(stopwords("en"), stopterms)) |>
  tokens_keep(pattern = "^[a-zA-Z]+$", valuetype = "regex")

toks_polsent_bigrams <- tokens_ngrams(toks_polsent, n = 2)

readr::write_rds(toks_polsent, here::here("data", "tokens", "filtered_tokens.rds"), compress = "gz")
readr::write_rds(toks_polsent_bigrams, here::here("data", "tokens", "filtered_bigrams.rds"), compress = "gz")

dfmat_polsent <- dfm(toks_polsent)
dfmat_polsent_bigrams <- dfm(toks_polsent_bigrams)

readr::write_rds(dfmat_polsent, here::here("data", "tokens", "dfmat_polsent.rds"), compress = "gz")
readr::write_rds(dfmat_polsent_bigrams, here::here("data", "tokens", "dfmat_polsent_bigrams.rds"), compress = "gz")
```