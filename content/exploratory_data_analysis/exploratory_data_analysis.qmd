---
title: "Exploratory Data Analysis"
author: "Keenan Smith"
---

Built with R Version `r getRversion()`

Guidance on R tidy text exploration provided by Silge, J & Robinson, D (2022). Text Mining with R (1st ed.). O'Reilly. [Link to Textbook](https://www.tidytextmining.com/index.html)

```{r}
#| output: false
#| label: Reading in initial Libraries
here::i_am("content/exploratory_data_analysis/exploratory_data_analysis.qmd")
library(here)
library(tidytable)

library(parallel)
all_cores <- parallel::detectCores(logical = FALSE)
cl <- parallel::makePSOCKcluster(all_cores)
doParallel::registerDoParallel(cl)
```

# Cleaning the Data

The following sections show pulling the data into R and then processing it into its final product. The `rvest` ingests text data row by row. Though R possesses the ability to analyze text data through grouping, I find that with the article data it is better to have one row, one article. Through the following steps I bring the data in, join it, add a bias classifier, and create the full corpus. I try to leave comments where applicable. 


# Exploratory Data Analysis

A majority of these techniques are using the code from Text Mining with R. 

```{r}
# Reading the data back in after processing


# Load Text Specific Libraries
library(tidytext)
# for future work
# library(text)

# stop words from the tidytext package
data("stop_words")
```

```{r}
# Initial look at word token data
bias_words <- text_condensed_full_corpus |>
  unnest_tokens(word, full_text) |>
  anti_join(stop_words) |>
  count(art_bias, word, sort = TRUE)

total_words <- bias_words |>
  group_by(art_bias) |>
  summarise(total = sum(n))

bias_words <- left_join(bias_words, total_words)

head(bias_words, 25)
```

```{r}
write_rds(bias_words, here("data", "text_words_freq_unfiltered.rds"),  
          "gz", compression = 9L)
```


## Term Frequency

```{r}
#| warning: false

ggplot(bias_words, aes(n/total, fill = art_bias)) +
  geom_histogram(show.legend = FALSE, bins = 5) +
  xlim(NA, 0.0009) +
  facet_wrap(~art_bias, ncol = 2, scales = "free_y")
```

```{r}
bias_tf_idf <- bias_words |>
  bind_tf_idf(word, art_bias, n)

bias_tf_idf |>
  select(-total) |>
  arrange(desc(tf_idf)) |>
  head(25)
```

```{r}
bias_tf_idf |>
  group_by(art_bias) |>
  slice_max(tf_idf, n = 15) |>
  ungroup() |>
  ggplot(aes(tf_idf, forcats::fct_reorder(word, tf_idf), fill = art_bias)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~art_bias, ncol = 2, scales = "free") +
  labs(x = "tf-idf", y = NULL)
```

```{r}
#| echo: false

write_rds(bias_tf_idf, here("data", "text_token_tf_idf_unfiltered.rds"),  
          "gz", compression = 9L)

rm(bias_words, bias_tf_idf)
```

After looking at the initial word token tf-idf data, we can see that there are alot of proper nouns and use of the thinktank or websites name. There are some interesting things to be seen in the initial data. Since this is a helpful technique for language classification, I plan on using this as the initial modeling vectorization for my non-deep learning modeling. 

This tf-idf chart also shows that their is a global nature to these proper nouns. This may be due to the nature of the sources selected and the amount of the corpus that is contributing to each side. I am currently working to expand the corpus to include more American centric think tanks. This will hopefully dull some of the global nature of the proper nouns used. However, this could lead to *different* questions like "do left wing sites choose to cover more international events than right wing?"

Fortunately, these data don't seem to have high valuation for numbers. This is different from my experience using the `NLTK` framework in Python on my initial look. However, I did my exploratory data analysis post modeling and under different conditions aka the deliverable was different. 

## n-grams
```{r}
bias_bigrams <- text_condensed_full_corpus |>
  unnest_tokens(bigram, full_text, token = "ngrams", n = 2) |>
  filter(!is.na(bigram))

bigrams_separated <- bias_bigrams |>
  lazy_dt() |>
  separate(bigram, c("word1", "word2"), sep = " ")

bigrams_filtered <- bigrams_separated |>
  filter(!word1 %in% stop_words$word) |>
  filter(!word2 %in% stop_words$word)

bigrams_united <- bigrams_filtered |>
  mutate(bigram = paste(word1, word2, sep = " ")) |>
  as_tibble()
```

```{r}
#| echo: false

rm(bigrams_filtered, bigrams_separated, bias_bigrams)
```


```{r}
bigram_tf_idf <- bigrams_united |>
  count(art_bias, bigram) |>
  bind_tf_idf(bigram, art_bias, n) |>
  arrange(desc(tf_idf))

head(bigram_tf_idf, 25)
```
```{r}
#| echo: false

rm(bigrams_united)
```


```{r}
bigram_tf_idf |>
  group_by(art_bias) |>
  slice_max(tf_idf, n = 15) |>
  ungroup() |>
  ggplot(aes(tf_idf, forcats::fct_reorder(bigram, tf_idf), fill = art_bias)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~art_bias, ncol = 2, scales = "free") +
  labs(x = "tf-idf", y = NULL)
```

n-grams characterize speech in phrases. In TF-IDF, this provides context. When a word token might capture one of the words in a proper noun or part of speech, n-grams allow us to see more of that context. This is a lighter approximation of word embedding, but where embedding and transformers are far more mathematically and theory based, n-gram analysis in this case is more based on intuition and subject matter expertise. Some of the bigrams present in the figure show that there are non-proper nouns that show up in the analysis. We could use these to draw some conclusions about what certain organizations like to talk about or what they don't like to talk about. It is certainly interesting that the Right-wing has a top 15 that is mainly proper nouns, whereas the left has more sayings or talking points. I don't want to draw too many conclusions however, because that is not the current scope of the question, the current is whether political speech can classify bias. Another interesting question would be whether or not one side spends time addressing the other side or vice versa. 

# NLP Technique

The NLP technique that I am using to solve my question is modeling via TF-idf vectorization. I think this suites my question well and I have already seen some promising initial tests prior to this class. My end goal of this capstone is to ultimately build a political lexicon by which to classify speech in the future. 

As this is part of a larger capstone project, I am currently adding and expanding my corpus. I am collecting links and will be scraping for more data soon. With the size of the current corpus, I may need to look at new data management techniques to speed the process along for exploratory data analysis. Luckily, with `dtplyr` I can do many of the data manipulation steps neccessary, but I may need to look at additional parallel processing to help.

I will also be expanding this out to look at deep learning techniques and embeddings. I already have some code which I can use to create my own embeddings specific to this corpus. I plan on looking at a bit more vectorization techniques prior to actual modeling. 

```{r}
write_rds(bigram_tf_idf, here("data", "text_bigram_tf_idf_unfiltered.rds"),  
          "gz", compression = 9L)
```

