---
title: "Exploratory Data Analysis"
author: "Keenan Smith"
editor_options: 
  chunk_output_type: console
---

Built with R Version `r getRversion()`

Guidance on R tidy text exploration provided by Silge, J & Robinson, D (2022). Text Mining with R (1st ed.). O'Reilly. [Link to Textbook](https://www.tidytextmining.com/index.html)

```{r}
#| output: false
#| label: Reading in initial Libraries
here::i_am("content/exploratory_data_analysis/exploratory_data_analysis.qmd")
library(tidytable)
library(ggplot2)
library(quanteda)
library(quanteda.textstats)
library(quanteda.textplots)
```

# ONLY RUN IF FIRST TIME

**Takes about 20 mins to get all data into R and tokenized.**

## Importing Corpus and Filtering

Eliminated Human Rights Watch since after initial inspection, the text is mainly about non-English subjects and represents a vast majority of the important Left Wing words. 

```{r}
require(DBI)
pol_parquet_db <- dbConnect(duckdb::duckdb(), dbdir = here::here("data","pol-parquet.duckdb"))

corpus_df <- dplyr::tbl(pol_parquet_db, "full_corpus") |>
  dplyr::collect() |>
  tidytable::as_tidytable() |>
  tidytable::mutate(
    short_source = as.factor(short_source),
    source_bias = as.factor(source_bias)
  ) |>
  tidytable::rename(text = full_text) |>
  tidytable::filter(length_text > 50)

DBI::dbDisconnect(pol_parquet_db, shutdown = TRUE)
```

```{r}
ggplot(corpus_df, aes(x = length_text, fill = short_source, color = short_source)) +
  geom_histogram(alpha = 0.5, position = "identity") +
  scale_x_log10(labels = scales::label_number_si())

ggplot(corpus_df, aes(x = length_text, fill = source_bias, color = source_bias)) +
  geom_histogram(alpha = 0.5, position = "identity") +
  scale_x_log10(labels = scales::label_number_si())

length_df <- corpus_df |>
  select(short_source, source_bias, length_text)

group_source <- corpus_df |>
  group_by(short_source) |>
  count(sort = TRUE)

group_bias <- corpus_df |>
  group_by(source_bias) |>
  count()

# MAKE HISTOGRAM PLOT
```


## Creating Corpus Object

Added Unique Document Names

```{r}
#| label: Quanteda Operations
corpus_polsent <- corpus(corpus_df)

docid <- paste(corpus_df$short_source,
               corpus_df$pull_index,
               corpus_df$source_bias,
               sep = "_")

docnames(corpus_polsent) <- docid

# All stopwords were selected by Initial Analysis of the Total Dataset
# there may be more as the Corpus is quite large but these had the biggest effect on the Data
additional_stopwords <- c("epop", "percision", "embedclicker", "textarea", "getembedcode", "footerright", "footerleft", "customjson", "attr", "thead", "styledtable", "roboto", "tbody", "monospace", "scopedstyledtable", "xaxis", "plotbands", "plotoptions", "chartinfo", "yaxistitle", "yaxismin", "yaxismax", "yaxisvisibility", "xaxistitle", "xaxismultipletitles", "xaxisunits", "xaxisplotbands", "defaultoffset", "orderby", "showdatalabels", "showfirstdatalabel", "showlastdatalabel", "decimalplaces", "heightadjustment", "epicharts", "showscatterlabels", "showscattermarkers", "showregressionline", "showregressionequation", "regressionlabel", "regressionslope", "regressionintercept", "chartdatadownload", "verticalalign", "f", "embed", "layout", "download", "chart", "copy", "facebook", "y", "x", "figure", "data", "null", "stacking", "photo", "height", "semihidden", "quot", "background", "align", "backgrounder", "swf", "istockphoto", "jquery", "image", "tweet", "id", "title", "browser", "unhidewhenused", "scribd", "post")
# Ensuring that Sources were removed from the Corpus
source_stopwords <- c("enterprise", "institute", "cato", "heritage", "foundation", "center", "mercatus", "manhattan", "cbpp", "discovery", "hoover", "claremont", "guttmacher", "jacobin", "commonwealth", "epi", "aei", "cap")

bigram_stopterms <- c("amp_amp", "percent_percent", "originally_appeared", "piece_originally", "cdata_cdata", "appeared_daily", "share_share", "right_top", "code_website", "share_twitter", "research_insights", "para_para", "n_n", "miss_research", "yes_yes", "enjoyed_sign", "excel_underlying", "legend_position", "type_legend", "visibility_type", "false_visibility", "enabled_true", "type_line", "miss_posts", "related_reading", "appeared_forbes", "posts_papers", "recently_highlights", "fellow_contributing", "papers_charts", "editor_city", "outlets_featured", "appeared_realclearmarkets", "act_ref", "fellow_follow", "grid_table", "name_grid", "code_var", "floating_false", "var_shared", "false_text", "hidden_enabled", "underlying_note", "line_false", "twitter_variety", "table_colorful", "top_calc", "tab_format", "bottom_horizontal", "display_tab", "bold_arial", "none_border", "none_linear", "buffon_rgba", "transparent_rgba", "color_transparent", "rgba_buffon", "table_accent")

bigrams_stopterms <- unique(unlist(strsplit(bigram_stopterms, "_")))

stopterms <- c(additional_stopwords, source_stopwords, bigrams_stopterms)

# toks_polsent_unfiltered <- tokens(corpus_polsent, remove_punct = TRUE, remove_symbols = TRUE, remove_numbers = TRUE)

toks_polsent <- tokens(corpus_polsent, remove_punct = TRUE, remove_symbols = TRUE, remove_numbers = TRUE) |>
  tokens_remove(pattern = c(stopwords("en"), stopterms)) |>
  tokens_keep(pattern = "^[a-zA-Z]+$", valuetype = "regex")

toks_polsent_bigrams <- tokens_ngrams(toks_polsent, n = 2)

readr::write_rds(toks_polsent, here::here("data", "tokens", "filtered_tokens.rds"), compress = "gz")
readr::write_rds(toks_polsent_bigrams, here::here("data", "tokens", "filtered_bigrams.rds"), compress = "gz")

dfmat_polsent <- dfm(toks_polsent)
dfmat_polsent_bigrams <- dfm(toks_polsent_bigrams)

readr::write_rds(dfmat_polsent, here::here("data", "tokens", "dfmat_polsent.rds"), compress = "gz")
readr::write_rds(dfmat_polsent_bigrams, here::here("data", "tokens", "dfmat_polsent_bigrams.rds"), compress = "gz")
```



# START HERE

# Reading In already Processed Document Feature Matrices & Token Objects

```{r}
toks_polsent <- readr::read_rds(here::here("data", "tokens", "filtered_tokens.rds"))
toks_polsent_bigrams <- readr::read_rds(here::here("data", "tokens", "filtered_bigrams.rds"))

dfmat_polsent <- readr::read_rds(here::here("data", "tokens", "dfmat_polsent.rds"))
dfmat_polsent_bigrams <- readr::read_rds(here::here("data", "tokens", "dfmat_polsent_bigrams.rds"))
```

```{r}
# Trimmed word Data for Frequency as well
dfmat_words_trim <- dfm_trim(dfmat_polsent, min_termfreq = 1000, termfreq_type = "rank")

# Grouping Words Based on Source Bias
dfmat_group <- dfmat_words_trim |>
  dfm_group(groups = source_bias)

# Trimmed Frequency to Make Data Easier to Manage
dfmat_bigrams_trim <- dfm_trim(dfmat_polsent_bigrams, min_termfreq = 1000, termfreq_type = "rank")

# Grouping Bigrams Based on Source Bias
dfmat_group_bigrams <- dfmat_bigrams_trim |>
  dfm_group(groups = source_bias)
```

# Using Quanteda to Analyze Data

## Proportion Vectorization of Individual Documents

```{r}
dfmat_polsent_prop <- dfm_weight(dfmat_words_trim, scheme = "prop")
```

## TF-IDF Vectorization of Individual Documents

This could be used for modeling but Grouping meets the research question

```{r}
dfmat_words_tfidf <- dfm_tfidf(dfmat_words_trim)
dfmat_bigrams_tfidf <- dfm_tfidf(dfmat_bigrams_trim)
```

## Grouping by Article Source

```{r}
dfmat_group_source <- dfm_group(dfmat_words_trim, groups = short_source) |>
  dfm_select(min_nchar = 2)
```

# Tidied Group Data on Bias

```{r}
convert_test <- convert(dfmat_words_trim, to = "data.frame")

dfmat_to_tidy <- function(dfmat) {
  tidytext::tidy(dfmat) |>
    tidytable::separate_wider_delim(document, delim = "_", names = c("short_source", "pull_index", "source_bias"), cols_remove = FALSE)
}

tidied_bigrams <- dfmat_to_tidy(dfmat_bigrams_trim) 
tidied_words <- dfmat_to_tidy(dfmat_words_trim)
```

```{r}
test <- tidied_bigrams |>
    pivot_wider(names_from = term, names_prefix = "term_",
              values_from = count, values_fill = 0)

test <- test |>
  mutate(short_source = as.factor(short_source),
         source_bias = as.factor(source_bias))

test |>
  group_by(source_bias) |>
  count()
```



```{r}
test_sparse <- tidied_bigrams |>
  tidytext::cast_sparse(source_bias, term, count)

test_rownames <- rownames(test_sparse)
```

## Frequency Analysis

```{r}
tstat_freq <- textstat_frequency(dfmat_bigrams_trim, n = 200, groups = source_bias)
tstat_freq

left_freq <- subset(tstat_freq, group == "left-wing")
right_freq <- subset(tstat_freq, group == "right-wing")
```

### Individual Bigram Frequency WordCloud

```{r}

wordcloud2::wordcloud2(left_freq, color = pals::ocean.thermal(100), minRotation = pi/6, maxRotation = pi/6, rotateRatio = 1)
wordcloud2::wordcloud2(right_freq, color = pals::ocean.oxy(100), minRotation = -pi/6, maxRotation = -pi/6, rotateRatio = 1)
```

```{r}
#| fig-width: 8
#| fig-height: 8
# create wordcloud
set.seed(132) # set seed for reproducibility
textplot_wordcloud(dfmat_group_bigrams, comparison = TRUE, max_words = 500,  color = c("blue", "red"))
textplot_wordcloud(dfmat_group, comparison = TRUE, max_words = 200, color = c("blue", "red"))
```

## Lexical Diversity Between Sources

```{r}
tstat_lexdiv <- textstat_lexdiv(dfmat_group_source)
tstat_lexdiv
```

```{r}
ggplot(tstat_lexdiv, aes(document, TTR)) + 
  geom_col(aes(fill = document)) +
  labs(x = "Article Sources",
       y = "Lexical Diversity") +
  ggthemes::scale_fill_tableau("Tableau 20") +
  ggthemes::theme_clean() +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1),
        legend.position = "none")

ggsave(here::here("content", "exploratory_data_analysis", "images", "art_source_lexical_diversity.png"))
```


## Collocation on Tokens

[Source for How to Do This](https://tutorials.quanteda.io/statistical-analysis/collocation/)

```{r}
tstat_col_caps <- tokens_select(toks_polsent, pattern = "^[A-Z]", 
                                valuetype = "regex", 
                                case_insensitive = FALSE, 
                                padding = TRUE) |>
                  textstat_collocations(min_count = 100)

tstat_col2 <- tokens_select(toks_polsent, pattern = "^[A-Z]", 
                                valuetype = "regex", 
                                case_insensitive = FALSE, 
                                padding = TRUE) |> 
              textstat_collocations(min_count = 100, size = 3)
```

## Clustering Based on Article Source

[Source for How to Do This](https://tutorials.quanteda.io/advanced-operations/twitter-user-similarity/)

```{r}
tstat_dist <- as.dist(textstat_dist(dfmat_group_source))
group_clust <- hclust(tstat_dist)

plot(group_clust)

ggdendro::ggdendrogram(group_clust, theme_dendro = FALSE) +
  labs(title = "Cluster Analysis for Article Source",
       x = "Article Sources",
       y = "")
ggsave(here::here("content", "exploratory_data_analysis", "images", "art_source_cluster.png"))
```