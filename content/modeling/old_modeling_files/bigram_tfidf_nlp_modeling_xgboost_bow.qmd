---
title: "xgboost NLP Modeling"
author: "Keenan Smith"
---

```{r}
#| output: false
#| label: Reading in initial Libraries
here::i_am("content/modeling/bigram_tfidf_nlp_modeling_xgboost.qmd")
library(tidymodels)

library(parallel)
all_cores <- parallel::detectCores(logical = FALSE)
cl <- parallel::makePSOCKcluster(all_cores)
doParallel::registerDoParallel(cl)

sparse_bp <- hardhat::default_recipe_blueprint(composition = "dgCMatrix")

docid_to_bias <- function(df) {
  df |>
    tidytable::separate_wider_delim(doc_id, delim = "_", names = c("short_source", "pull_index", "source_bias"), cols_remove = TRUE) |>
    tidytable::select(-short_source, -pull_index)
}
```

```{r}
bigram_bow <- tidytable::fread(here::here("data", "model_data", "bigram_df.csv.gz"))

rownames(bigram_bow) <- bigram_bow$doc_id
```

# Getting Classification out of Doc ID

```{r}
bigram_bow <- docid_to_bias(bigram_bow)
```

# Data Split

```{r}
set.seed(2023)
bow_split <- initial_split(bigram_bow, strata = source_bias)

bigram_train <- training(tfidf_split)
bigram_test <- testing(tfidf_split)
bigram_folds <- vfold_cv(bigram_train)
```

# Engine

```{r}
library(xgboost)

tune_spec <- boost_tree(
  trees = 1000,
  tree_depth = tune(), 
  min_n = tune(),
  loss_reduction = tune(),
  learn_rate = tune()) |>
  set_mode("classification") |>
  set_engine("xgboost")
```

# Tune Grid

```{r}
xgboost_params <- 
  dials::parameters(
    min_n(),
    tree_depth(),
    learn_rate(),
    loss_reduction()
  )

xgb_grid <- 
  dials::grid_max_entropy(
    xgboost_params, 
    size = 60
  )
```

# Recipes

```{r}
text_rec <- recipe(source_bias ~ ., data = bigram_train)
```

# Workflows

```{r}
tune_wf <- workflow() |>
  add_recipe(text_rec, blueprint = sparse_bp) |>
  add_model(tune_spec)
```

```{r}
tune_rs <- tune_grid(
  tune_wf,
  bigram_folds,
  grid = xgb_grid,
  control = control_resamples(save_pred = TRUE)
)

readr::write_rds(tune_rs, "model_results/xgboost_tune_data.rds", compress = c("gz"))
```

```{r}
xgboost_rs_metrics <- collect_metrics(tune_rs)
tidytable::fwrite(xgboost_rs_metrics, here::here("content", "modeling", "model_results", "xgboost_rs_metrics_bigrams.csv"))

show_best(tune_rs)
```

```{r}
tune_rs |>
  collect_metrics() |>
  filter(.metric == "roc_auc") |>
  select(mean, min_n:loss_reduction) |>
  pivot_longer(min_n:loss_reduction,
               values_to = "value",
               names_to = "parameter"
  ) |>
  ggplot(aes(value, mean, color = parameter)) +
  geom_point(alpha = 0.8, show.legend = FALSE) +
  facet_wrap(~parameter, scales = "free_x") +
  labs(x = NULL, y = "AUC")
```


```{r}
chosen_auc <- tune_rs |>
  select_best(metric = "roc_auc")

final_xgboost <- finalize_workflow(tune_wf, chosen_auc)
```

```{r}
fitted_xgboost <- fit(final_xgboost, bigram_train)
```

```{r}
library(vip)

vip_xgboost <- fitted_xgboost |>
  extract_fit_parsnip() |>
  vi()

vip_xgboost |>
  slice_head(n = 20) |>
  ggplot(aes(x = Importance, y = Variable)) +
  geom_col() +
  scale_x_continuous(expand = c(0, 0)) +
  labs(y = NULL)


ggsave("variable_importance_plot_xgboost.png")
```

```{r}
last_fit_xgboost <- last_fit(final_xgboost, tfidf_split)

xgboost_metric <- last_fit_xgboost |>
  collect_metrics()

tidytable::fwrite(xgboost_metric, "model_results/xgboost_test_metrics.csv")

last_fit_xgboost |>
  collect_predictions() |>
  roc_curve(source_bias, estimate = `.pred_left-wing`) |>
  ggplot(aes(x = 1 - specificity, y = sensitivity)) +
  geom_line(size = 1.5, color = "midnightblue") +
  geom_abline(
    lty = 2, alpha = 0.5,
    color = "gray50",
    size = 1.2
  )

ggsave("roc_curve_xgboost.png", width = 4, height = 4)
```

