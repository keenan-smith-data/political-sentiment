---
title: "Classical NLP Modeling"
author: "Keenan Smith"
---

```{r}
#| output: false
#| label: Reading in initial Libraries
here::i_am("content/exploratory_data_analysis/exploratory_data_analysis.qmd")

library(parallel)
all_cores <- parallel::detectCores(logical = FALSE)
cl <- parallel::makePSOCKcluster(all_cores)
doParallel::registerDoParallel(cl)
```

```{r}
require(DBI)
pol_parquet_db <- dbConnect(duckdb::duckdb(), dbdir = here::here("data","pol-parquet.duckdb"))

corpus_df <- dplyr::tbl(pol_parquet_db, "full_corpus") |>
  dplyr::collect() |>
  tidytable::as_tidytable() |>
  tidytable::mutate(
    short_source = as.factor(short_source),
    source_bias = as.factor(source_bias)
  ) |>
  tidytable::rename(text = full_text)

corpus_df <- corpus_df |>
  tidytable::filter(length_text > 50)

DBI::dbDisconnect(pol_parquet_db, shutdown = TRUE)
```

# Prepping for Modeling

```{r}
library(spacyr)
spacy_initialize(model = "en_core_web_sm")
```


```{r}
library(textrecipes)
```

```{r}
rec <- recipe(source_bias ~ text, data = corpus_df) |>
  step_tokenize(text, engine = "spacyr") |>
  step_ngram(text, num_tokens = 2) |>
  step_tokenfilter(text, min_times = 20) |>
  step_tf(text)
```

```{r}
start_time <- Sys.time()
corpus_baked <- rec |>
  prep() |>
  bake(new_data = NULL)
end_time <- Sys.time()

print(end_time - start_time)
```

