---
title: "randomforest NLP Modeling"
author: "Keenan Smith"
editor_options: 
  chunk_output_type: console
---

```{r}
#| output: false
#| label: Reading in initial Libraries
here::i_am("content/modeling/bigram_nlp_modeling_randomforest_lsa.qmd")

library(parallel)
all_cores <- parallel::detectCores(logical = FALSE)
cl <- parallel::makePSOCKcluster(all_cores)

# Script to Ensure Data Reproducibility
source(here::here("R", "data_prep_modeling_lsa.R"), echo = TRUE)
```

# Recipes

```{r}
sparse_bp <- hardhat::default_recipe_blueprint(composition = "dgCMatrix")

text_rec <- recipe(source_bias ~ ., data = bigram_train)
```

# Engine

```{r}
library(ranger)

tune_spec <- rand_forest(
  mtry = tune(), # Randomly Selected Predictors
  trees = tune(), # Trees
  min_n = tune() # Minimal Node Size
  ) |>
  set_mode("classification") |>
  set_engine("ranger")
```

# Tune Grid

```{r}
randomforest_params <- 
  dials::parameters(
    finalize(mtry(), bigram_train),
    trees(),
    min_n()
  )

randomforest_grid <- 
  dials::grid_max_entropy(
    randomforest_params, 
    size = 10
  )
```

# Workflows

```{r}
tune_wf <- workflow() |>
  add_recipe(text_rec, blueprint = sparse_bp) |>
  add_model(tune_spec)
```

# Metric Set

```{r}
class_metrics <- metric_set(sens, spec, recall, precision, f_meas, accuracy, roc_auc, pr_auc)
```

# Hyperparameter Tuning

```{r}
doParallel::registerDoParallel(cl)
tune_rs <- tune_grid(
  tune_wf,
  bigram_folds,
  grid = randomforest_grid,
  metrics = class_metrics,
)

readr::write_rds(tune_rs, "model_data/tune_results_randomforest_lsa.rds")
parallel::stopCluster(cl)
```

# Hyperparameter Metrics

```{r}
randomforest_rs_metrics <- collect_metrics(tune_rs)
tidytable::fwrite(randomforest_rs_metrics, here::here("content", "modeling", "model_results", "randomforest_rs_metrics_lsa_bigrams.csv"))

show_best(tune_rs)
```

## Metric vs. Lambda

```{r}
tune_rs |>
  collect_metrics() |>
  filter(.metric == "roc_auc") |>
  select(mean, min_n:loss_reduction) |>
  pivot_longer(min_n:loss_reduction,
               values_to = "value",
               names_to = "parameter"
  ) |>
  ggplot(aes(value, mean, color = parameter)) +
  geom_point(alpha = 0.8, show.legend = FALSE) +
  facet_wrap(~parameter, scales = "free_x") +
  labs(x = NULL, y = "AUC")

ggsave("plots/cv_metrics_randomforest_lsa.png")
```

# Selecting Best ROC-AUC to tune Model

```{r}
chosen_auc <- tune_rs |>
  select_best(metric = "roc_auc")

final_randomforest <- finalize_workflow(tune_wf, chosen_auc)
```

# Fitting Model on Training Data

```{r}
fitted_randomforest <- fit(final_randomforest, bigram_train)
```

# Variable Importance Examination

## Variable Importance Plot

```{r}
library(vip)

vip_randomforest <- fitted_randomforest |>
  extract_fit_parsnip() |>
  vi()

vip_randomforest |>
  slice_head(n = 20) |>
  ggplot(aes(x = Importance, y = Variable)) +
  geom_col() +
  scale_x_continuous(expand = c(0, 0)) +
  labs(y = NULL)

ggsave("plots/variable_importance_plot_randomforest_lsa.png")
```

```{r}
last_fit_randomforest <- last_fit(final_randomforest, lsa_split)

randomforest_metric <- last_fit_randomforest |>
  collect_predictions() |>
  class_metrics(source_bias, estimate = .pred_class, `.pred_left-wing`)

tidytable::fwrite(randomforest_metric, "model_results/randomforest_test_metrics_lsa.csv")

last_fit_randomforest |>
  collect_predictions() |>
  roc_curve(source_bias, `.pred_left-wing`) |>
  autoplot()

ggsave("plots/roc_curve_randomforest_lsa.png")

last_fit_randomforest |>
  collect_predictions() |>
  conf_mat(source_bias, .pred_class) |>
  autoplot(type = "heatmap")

ggsave("plots/confusion_matrix_randomforest_lsa.png")
```

