---
title: "Tokenizing Political Language to Determine Bias"
author: "Keenan Smith"
format:
  revealjs:
    theme: serif
    logo: ncstate-type-2x2-red.png
    footer: Political_Sentiment_Capstone_Smith_Keenan 
    width: 1920
    height: 1080
    embed-resources: true
    self-contained-math: true
---

# Overview

:::: {.columns}
::: {.column width="50%"}

-   Introduction
    -   Research Question, Political Ideology, NLP
-   Methods
    -   The Data
    -   Programming Language
    -   Data Collection

:::

::: {.column width="50%"}

-   Results
    -   EDA
    -   Model Results
-   Further Work
-   Questions

:::
::::

## Research Question

A **Data Science** Project that examines the question *"Does Political Speech indicate classical right-left political bias?"*

:::: {.columns}
::: {.column width="55%"}

- The Corpus consists of Ideologically Identified Sources
  - Primarily U.S. Think Tanks
    - Specifically Chosen for Bias
  - Data are Scraped from the Web
  - These Data are in English
- Classical Machine Learning Techniques are used for Classification
  - Transformers (The tech behind GPT) are goals for the future
- Source Code is Located on [Github](https://github.com/keenan-smith-data/political-sentiment)

:::

::: {.column width="45%"}

**Why?**

- We live in a politically charged and active time in the United States
- I like history, politics, and Natural Language
- How we communicate is meaningful and important
- The US Supreme Court uses Originalism and Textualism in their decisions

:::
::::

::: {.notes}
My interests are all the things you are not supposed to talk about at Thanksgiving
Transformers are the current highest form for NLP modeling but require much more computing power
:::

## Political Ideology

"Ideologies have for different individuals, different degrees of appeal. A matter that depends upon the individuals needs and the degree to which these needs are being satisfied or frustrated." (Adorno et al. 1950)

- The study of political ideology is vast and complex (Feldman and Johnston, 2013)
  - Construction of political parties are equally complicated and vast (Mayer, 2018; Mudge, 2018)
- This project chooses intentionally to focus on a wide corpus of data from left and right
- Clustering of the Sources used shows this is a complicated topic
  - Further work should focus on this diversity and complexity
  
![Credit: Wikipedia 118th US Congress Makeup](images/118th_US_House_of_Representatives.png)

::: {.notes}
Speaker notes go here.
:::

## Natural Language Processing {.smaller}

"the application of computer science to the **analysis, synthesis, and comprehension"** of written and spoken language" (Oxford Dictionary)

:::: {.columns}
::: {.column width="55%"}

- Mixing Data Science and how we use Natural Language
- NLP use linguistic tools to build *features* out of langauge
- These features are then transformed into mathematical vectors in order to model
  - Tokens (units of language)
    - Characters, Words, n-grams, Sentences, Paragraphs
  - Term Frequency
  - Term Frequency Inverse Document Frequency (Tf-idf)
  - Embeddings (not used in this project)
  
:::

::: {.column width="45%"}
![Credit: Xoriant](images/nlp_picture.png)
:::
::::

- There are many other applications such as sentiment analysis and topic modeling to name a few

::: {.notes}
Speaker notes go here.
:::

# Methods

## The Data

:::: {.columns}
::: {.column width="65%"}
- Bias
  - Selected Data Sources on Availability of Data
  - Trimmed as much as reasonably practical from Large Sources
  - Stopwords chosen based on EDA and Early Models
    - Removed artifacts left behind from scrape
- Data Integrity
  - Data Scraped Straight Into SQL Database
  - Strict Type Adherence
  - Uniqueness of Links Checked through Final Corpus selection
  - No Text, No Input into DB

:::

::: {.column width="35%"}

- Metadata
  - Article Link `art_link` VARCHAR
  - Article Date `art_date` DATE
  - Article Author(s) `art_author` VARCHAR
  - Article Source `art_source` VARCHAR
  - Article Bias `art_bias` VARCHAR
- Data
  - Text VARCHAR

:::
::::

::: {.notes}
This needs some tables
:::

## Source Table

```{r}
#| echo: false
source_table <- tidytable::fread(here::here("data", "sources", "source_table.csv"))
kableExtra::kbl(source_table) |>
  kableExtra::kable_styling(bootstrap_options = "striped", font_size = 24, latex_options = "HOLD_position")
```

## Data Sample {.scrollable}

```{r}
sample_text <- tidytable::fread("sample_corpus.csv")
kableExtra::kbl(sample_text) |>
  kableExtra::kable_styling(bootstrap_options = "striped", font_size = 16)
```

::: {.notes}
Table of Sample Data
:::

## The Data Prior to Modeling

- The Corpus is tokenized using the Quanteda framework of libraries
  = Bigrams were chosen due to their ability to contain more lexical scope information
  - Symbols, Numbers, and common Stopwords were excluded from the Corpus
  - Specific Stopwords were chosen via EDA and early LASSO Classification
    - Many of these are leftover web artifacts from scraping being an imperfect science
    - Others are common terms used in articles ("originally_published", "see_also")
    - Lastly, proper nouns specific to certain think tanks (Organization Names, Sponsors)
- This Corpus is Large consisting of 148,703 documents
  - 141,840,157 total words
  - 3000 predictors chosen based on Frequency, vectorized using TF-IDF
- The Data are split 75/25 stratifying on bias
  - 111,526 documents in the training set
  - 37,177 documents in the testing set
  - Random Seed state is set to 2023 in every model for reproducibility

::: {.notes}
Stopwords were removed to the best of my ability. Many times these were obvious, but most of the time they were pulled out after others were removed.
Lastly, certain stopwords were not removed until initial modelling was completed as they weighed heavily towards identifying bias. This process is imperfect.
Early on, the corpus had to be retokenized and saved to disk which took 10-20 mins per cycle.  
:::

## Process

![Technical Flow Chat](Flow Chart.drawio.png)

::: {.notes}
Speaker notes go here.
:::

# Results

## Data Source in Length of Text {.scrollable}

```{r}
source_count <- tidytable::fread("source_count_filtered.csv")
kableExtra::kbl(source_count) |>
  kableExtra::kable_styling(bootstrap_options = "striped", font_size = 28)
```


## Exploratory Data Analysis {.smaller}

![Histogram Length Text by Bias](images/corpus_histogram_filtered_bias.png)

::: {.notes}
Speaker notes go here.
:::

## Exploratory Data Analysis {.smaller}

![Histogram Length Text by Bias](images/corpus_histogram_filtered_source.png)

## Bigram Wordclouds

:::: {.columns}
::: {.column width="50%"}

![](images/left_freq_wordcloud_bigram.png)

:::

::: {.column width="50%"}

![](images/right_freq_bigram_wordcloud.png)

:::
::::

## Model Results  {.scrollable}

::: {.panel-tabset}

### Tab A

:::: {.columns}
::: {.column width="55%"}

- 3000 of the Most Frequent Bigrams Selected
  - Bigrams contain more information than Word Tokens
  - Early Modeling showed Bigrams had better results
- Regularized LASSO Regression Used so far

```{r}
lasso_test_metrics <- tidytable::fread("lasso_test_metrics.csv")
kableExtra::kbl(lasso_test_metrics) |>
  kableExtra::kable_styling(bootstrap_options = "striped", font_size = 28)
```

:::

::: {.column width="45%"}

![](images/variable_importance_plot.png)

:::
::::

### Tab B (ROC Curve)

![](images/roc_curve_lasso.png){width=40%}

:::

::: {.notes}
Speaker notes go here.
:::

## Conclusion & Remarks

- Results appear to be promising
  - An Accuracy of 82% and ROC AUC of 90% is excellent for LASSO Regresssion
  - Results are Relatively Easy to Interpret
- The Dataset is Large but not Unwieldy for others to Utilize
- The R Packages Utilized are Manageable and Easy to Learn
- Scraping is Easy to Replicate but Hard to find good sources

# Questions
