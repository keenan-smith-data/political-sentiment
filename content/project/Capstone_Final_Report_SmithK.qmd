---
title: "Tokenizing Political Language to Determine Bias"
subtitle: "EM675 Final Report Submission"
author: "Keenan Smith"
date: "19 Apr 2023"
abstract: ""
bibliography: references.bib
format:
  pdf:
    papersize: letter
    linestretch: 1
    fontsize: 12pt
    number-sections: true
    documentclass: article
    classoption: [titlepage, onecolumn]
    geometry:
      - top=30mm
      - left=20mm
    colorlinks: true
editor_options: 
  chunk_output_type: inline
---

# Introduction

With the populist election of Donald Trump in 2016, the record voter turn-out for the election of 2020, and the overturning of nearly 50 years of precedent in the Dobb's decision [@dobbs], we are currently living in one of the most politically active moments in United States political history [@politicalcharge]. At the same time, the invention and wide acceptance of the internet, the 24 hour news-cycle, and social media networks such as Twitter and Facebook, the volume of poltical speech is arguably greater than at any time in American History. From this basis, this project aims to look at political speech in a data-centric approach. It uses modern Natural Language Processing (NLP) techniques with classical modeling to ask the question "Does Political Speech Indicate Bias?"

This question is interesting not just from a personal interest perspective, but also because the United States Supreme Court utilizes a legal method called "Textualism" combined in some cases with "Originalism" to decide on issues regarding U.S. law. Textualism as defined by Constitution Annotated is "A mode of legal interpretation that focuses on the plain meaning of the text of a legal document. Textualists usually believe there is an objective meaning of the text, and they do not typically inquire into questions regarding the intent of the drafters, adopters, or ratifiers of the Constitution and its amendments when deriving meaning from the text." [@textualism] With this as a baseline, the author of this paper decided to look at the plain text of several think-tanks and news sources to determine whether the most commonly used terms in these texts could be accurately modelled to determine political bias.

This NLP project utilizes a primarily English corpus with minimal Spanish entries. This project focuses on the United States as much as reasonably practical. Some sources cover international issues more than others and some are located outside the United States. Due to the nature in which the text was collected, further multi-classification techniques would be required to further classify documents into specific categories. 

## Political Ideology

Political Ideology is a complex and nuanced subject. It is loosely defined as left and right wing by numerous news outlets and the public. However, this loose definition is often found to be lacking in precision. An study by Feldman and Jonston found using Latent Class Analysis, six distinct classes of individuals when measured against economic and social questions. {@feldman_2013} Adorno defined ideology as "Ideologies have for different individuals, different degrees of appeal. A matter that depends upon the individuals needs and the degree to which these needs are being satisfied or frustrated." [@adorno] Political parties are equally complex and nuanced with several actors and social movements. 

It is important to highlight this because the author classifies the data based on traditional right-wing and left-wing denominations. In the exploratory data analysis stage of this project, it was found that the overal corpus of text analyzed found that, based on unsupervised cluster analysis and lexical diversity analysis, the overall differences between the sources overall was not too disimilar. See Figure 1.  However, this is not part of the scope of this project and therefore is categorized for future work at a later date. 

![Cluster Analysis of Article Sources](images/art_source_cluster.png){width=80%}

## The Research Question

Lastly, it is important to define the full scope of the project before going forward. This project aims to answer the question "Does political speech indicate right-wing or left-wing political bias?" This is done using a web-scraped corpus of text from 21 think tanks and 1 news-source. The full corpus of text contains 148,703 documents that total 141,840,157 total words. The sources were selected using AcademicInfluence.com's Top Influential Think Tanks for 2023 [@thinktank] and filtered into right-wing and left-wing using Academicinfluence.com's determination as well as supporting research. A more thorough definition of the corpus of text will be given in the following section. [@mayer_party; @mudge_parties]

# Methods

## The Data

### Sources

Table 1 ^[Cato Institute is classified "libertarian", research by Public Religion Research Institute shows that a plurality of libertarians align themselves with right-wing parties [@libertarians]] ^[Sources are classified as Conservative as right-wing and liberal and progressive as left-wing [@thinktank]] shows the sources and biases utilized to build the corpus of text.

```{r}
#| echo: false
source_table <- tidytable::fread(here::here("data", "sources", "source_table.csv"))
kableExtra::kbl(source_table) |>
  kableExtra::kable_styling(bootstrap_options = "striped", font_size = 10, latex_options = "HOLD_position")
```

### Webscaping

These data are collected using the webscraping libraries `rvest` and `RSelenium`. These libraries and the avaliablility of the data to be scraped greatly determined which sources were included in the final corpus. Links were obtained using two methods of collection, sitemap XML data and the `linkchecker` Application Programming Interface (API). Links are checked several times for uniqueness to ensure zero duplication. The links were then cleaned widely into articles and blog posts with some reports being included in the overall corpus. This was done by filtering first on URL parsing and then by random spot checking of the links. After viable links were filtered, correct HyperText Markup Language (HTML) and Cascading Style Sheets (CSS) tags were manually selected for each link type to ensure the correct data were pulled from the web. The data are then stored in a relational database (`duckdb`) utilizing Structured Query Language (SQL) to ensure data integrity and correct typing of the data. 

### Data Structure and Type

Each article in the corpus contributed five metadata fields and one text data field. The five metadata fields are:

- Article Link `art_link` SQL data type VARCHAR PRIMARY KEY
- Article Date `art_date` SQL data type DATE
- Article Author(s) `art_author` SQL data type VARCHAR
- Article Source `art_source` VARCHAR
- Source Bias `source_bias` VARCHAR

The text data is classified as a SQL type VARCHAR.

If an article contained no viable text, it was not imported into the database and thus was excluded from the final corpus. The most important data for this project are Article Link, Article Date (for filtering), Source Bias and, the text. 

### Corpus Creation

```{r}
#| echo: false

bias_count <- tidytable::fread(here::here("content", "project", "bias_count_filtered.csv"))
kableExtra::kbl(bias_count) |>
  kableExtra::kable_styling(bootstrap_options = "striped", font_size = 10, latex_options = "HOLD_position")
```

To create the corpus, an analysis of the number of characters were performed to limit bias in the data. Some sources yielded more data than other sources due to the nature of content creation. The original data contained XX% right-wing and XX% left-wing. The corpus was then trimmed using the 90% quantile on American Enterprise Institute and 92% quantile on Heritage Foundation. This balanced the final corpus to 51% right-wing and 49% left-wing. Figure 2 shows the histogram of the final corpus in length of characters. As the data shows, there are more "mid-length" articles for right-wing sources but more "long" articles for left-wing. 

![Histogram of Final Corpus](images/corpus_histogram_filtered_bias.png)

The corpus is then tokenized into bigrams [@tidytext] (a collection of two words e.g. United States, rather than United, and States) using the `Quanteda` library. Bigrams were chosen due to their ability to contain more information than word tokenization. As the results will show, bigrams are particularly interesting in regards to political text. However, the trade-off is that bigrams are more computationally heavy than word tokens. [@tidytext] [@smltar] Another important aspect of text data is removing commonly used words. These words are often known as "stopwords". [@tidytext] Also, in a specialized corpus such as the one used in this project, there are also specialized "stopwords" that can be added to remove words that are not of interest in the analysis or that could bias the result based on artifacts from the webscrape. This project uses an extensive list of corpus specific stopwords found through Exploratory Data Analysis and Early Modeling. A copy of this list can be obtained by emailing the author. 

The corpus was filtered to only include data that was published on or after January 1st, 2010. This date was chosen due to the United States Mid-term elections of 2010 which highlighted a reaction to the first Obama administration [@rep_right] and the election of a Republican majority to both houses of congress. [@midterm_2010] Other date ranges could be subjects of further analysis.

### Text Vectorization and Feature Selection

In order to be modelled, text must be vectorized into a numeric format. [@smltar] Traditional methods of text vectorization are Term Frequency (tf) and Term Frequency Inverse Document Frequency (tf-idf). 

Term Frequency is defined as how frequently a word appears in a document. The inverse document frequency measures how frequent a word is in a unique to the document context. [@tidytext]

$$
tfidf = f_{i,j} / \sum_{i'\in j} f_{i',j} \space \cdot \space log(\frac{N}{df_i})
$$

where $f_{i,j}$ is number of occurences of i in j, $df_i$ is the number of documents containing i, and N is the total number of documents.

The text was vectorized using the `quanteda` library first using Term Frequency. [@quanteda] The top 10,000 bigrams in the corpus were then selected and transformed into a tf-idf vectorization. Text data, in particular, has a large sparse set of predictors [@tidytext]. In order to reduce the number of predictors from 10,000 to 400, Latent Semantic Analysis (LSA) is utilized. LSA uses singular value decomposition to perform dimensionality reduction on the number of predictors. [@latent_medium] LSA mathematically groups words together that are semantically similar. LSA has the advantage of dimensionality reduction and of grouping semantically similar words together into a single vector. However, like any dimsionality techniques [@islr], some information is lost in the process and bigram positional information is lost. In the context of this project, these drawbacks are considered minimal for the potential gains of reducing the number of features. It also grants insight into words that are semantically similar that otherwise would not have been found using traditional tf-idf vectorization.

Additionally, the top 4,000 bigrams were selected and vectorized using tf-idf and were used to model in addition to LSA. This method provides a bit more transparency to which variables are important and is easier to tokenize and analyze new data since LSA cannot fully reproduce the original feature matrix. 

#### Data Bias

It is important to note bias inherent in the data itself. Due to the nature of webscraping and the creation of text data by the sources used here, there are some sources that produce far more articles than others. This is attempted to be accounted for by selecting a large variety of sources as well as collecting a large amount of text data. However, this is not a perfect system and there are some sources that may influence the final result. Some think tanks may specialize in some areas of political policy more than others. In a larger scoped project, this would be discussed at length within this report, however, the author has decided that this analysis would be outside of the project scope but an activity for future work with this corpus. 

## Modelling

### Modelling Methodology

Once the corpus is vectorized, features are selected and LSA is performed, the data are readied to be modelled. The following section will highlight the models selected and used in this project. The models were chosen based on their ability to work with text data and best practices. No deep learning models were used in this project, however, it should be noted that deep learning methods, particularly those based on "transformers" are the state-of-the-art tools in text analysis at the time of this report. [@transformers_nlp] 

#### Models

The models that were chosen to model this question were:

- LASSO Logisitic Regression 
  - Utilized mainly for its ability to do feature selection and ease of explanation [@islr]
  - A good classical model for text [@smltar]
  - Ability to have hyperparameter tuning [@tmwr]
- Naive Bayes Classifier
  - Generally good start for text classification [@smltar]
  - A relatively simple model based on Bayes Thereom [@islr]
  - $Pr(Y = k | X = x) = \frac{\pi_i * f_k(x) = f_{k1}(x_1) * \dots * f_{kp}}{\sum^K_{k=1} \pi_l * f_{l1}(x_1) * \dots * f_{lp}}$
- Random Forest Classification
  - A Decision Tree Model which is easy to understand and train [@islr]
  - Have the ability to decorrelate data by randomly splitting predictors [@islr]
  - Ability to have hyperparameter tuning [@tmwr]

#### Hyperparameter Tuning

The models were training using the R programming language using the `tidymodels` framework to ensure consistentcy across modeling techniques. Using this framework allows for ease of hyperparameter tuning. Hyperparameter tuning is utilized to try to achieve higher performance metrics in modeling. [@smltar] For LASSO Logistic Regression, $\lambda$ was optimized over 10 possible values. For Random Forest, the number of trees, number of variables within one node, and the randomly selected predictors were selected for hyperparameter tuning. To see the specifics of using `tidymodels` to model data, Tidy Modeling with R is the recommended resource. [@tmwr]


# Results



## Exploratory Data Analysis



## Model Results



# Further Work



# References

::: {#refs}
:::
