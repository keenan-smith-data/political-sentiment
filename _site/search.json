[
  {
    "objectID": "project_charter.html",
    "href": "project_charter.html",
    "title": "Project Charter",
    "section": "",
    "text": "Who is the client, what business domain the client is in.\nClients\n\nNCSU MEM Faculty\nMyself\nNCSU MEM Cohort\nThe General Public\n\nDomain\n\nNatural Language Processing\nPolitics\nHistory\nBias\n\nWhat business problems are we trying to address?\nThe project is addressing the current political climate in the United States of America. We currently live in an information rich and ideological polarized state specifically in the U.S. This project is to determined whether intellectual political speech can be accurately modeled to determine political bias.\n\n\n\n\n\nI will be building a data science solution that encompasses a large corpus of political speech web scraped from top think tanks with clear ideological leanings. This project will encapsulate the entire data science spectrum from data collection, data cleaning, data exploration, data modeling, and model deployment.\nData has already been collected from 4 sources and preliminary data modeling has occured. The lessons learned from that initial analysis will be pulled through into this project.\n\nAdditional data will be acquired through similar means as the first 4 data sources.\n\nThe end product will most likely be a series of deployed models using the vetiver library\nIf there is additional time, I will attempt to create a shiny application that allows ease of access to the models and the data for stakeholders to explore but this may go out of scope as time permits.\n\n\n\n\n\nWho are on this project:\n\nStudent:\n\nKeenan Smith\n\nAdvisor:\n\nDr. Brandon McConnell\n\nFaculty Committee\n\n\n\n\n\n\nQualitative objectives\n\nAt least 4 deploy-able models using the Vetiver library that have been trained and optimized using grid search to find the highest accuracy model\nThe modeling and code must be left in a state where it is able to be redeployed with an expanded corpus\nThe hooks must be left so that there is potential for a generative model in the future\n\nWhat is a quantifiable metric (e.g. reduce the fraction of users with 4-week inactivity)\n\nAt least one model with a classification accuracy of 70% or more (This is subject to change as modeling is performed)\n\nWhat is the baseline (current) value of the metric?\n\nThe current baseline for this project is the null hypothesis that political speech does not indicate bias\n\nHow will we measure the metric? (e.g. A/B test on a specified subset for a specified period; or comparison of performance after implementation to baseline)\n\nThe metric will be measured by observing an accuracy score above 70%\n\n\n\n\n\n\nThis is a loose definition of the current plan\nBusiness Understanding\n\nDefining Objectives which is encapsulated in this project charter\nIdentifying Data Sources\n\nCurrently there are 4 but this will be expanded\nThe criteria is ease of the ability to scrape a large corpus of articles from the think tank, the explicit nature of the political bias present in popular culture, the ability to get the corpus into similar data as the currently acquired corpus\n\n\nData Acquisiton and Understanding\n\nIngest the Data into R and the model workflow\nWriting Data cleaning functions to clean the data prior to vectorization\nExplore the data using a variety of popular NLP vectorization techniques\n\nModeling\n\nFeature Engineering and Model Selection\nModeling the data using lessons learned from Exploration of the data\nEvaluating the Model\nAs a note, this project will utilize the tidymodels framework as much as possible as it uses a unified syntax across a variety of popular machine learning algorithms to output easily understood metrics and visualizations\n\nDeployment\n\nDeployed the selected models using the vetiver library.\nOptionally deploy the model using the plumber library to create an API for others to use the models\n\nThe key areas that I foresee that will take the most time is data acquisition and understanding. Using prior experience in this area and experience from others, data acquisition and cleaning the data take up to 70% of the work of a data analysis project. A project plan will be developed using project planning software to help plan this project in more detail for the stakeholders.\n\n\n\n\n\nData\n\nThe data will be provided via webscraping using the rvest package in R\nThe list of think tanks is currently being finalized at this current edition of the project charter\n\n\nWhat tools and data storage/analytics resources will be used in the solution e.g.,\n\nCurrently the data will be stored as CSV or .rda compressed data files\nSQLite will be explored as a way to store the data for deployment\n\nHow will the models be operationalized?\n\nThe models will be deployed using the vetiver library and possibly expanded to an API using the plumber library\n\n\n\n\n\n\nHow will we keep in touch? Weekly meetings?\n\nThere will be weekly update emails at a minimum to provide an update of current status. In-person or zoom meetings will be as required but at least occur on a bi-weekly basis."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "political-sentiment",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "project_charter_pdf.html",
    "href": "project_charter_pdf.html",
    "title": "Project Charter",
    "section": "",
    "text": "Who is the client, what business domain the client is in.\nClients\n\nNCSU MEM Faculty\nMyself\nNCSU MEM Cohort\nThe General Public\n\nDomain\n\nNatural Language Processing\nPolitics\nHistory\nBias\n\nWhat business problems are we trying to address?\nThe project is addressing the current political climate in the United States of America. We currently live in an information rich and ideological polarized state specifically in the U.S. This project is to determined whether intellectual political speech can be accurately modeled to determine political bias.\n\n\n\n\n\nI will be building a data science solution that encompasses a large corpus of political speech web scraped from top think tanks with clear ideological leanings. This project will encapsulate the entire data science spectrum from data collection, data cleaning, data exploration, data modeling, and model deployment.\nData has already been collected from 4 sources and preliminary data modeling has occured. The lessons learned from that initial analysis will be pulled through into this project.\n\nAdditional data will be acquired through similar means as the first 4 data sources.\n\nThe end product will most likely be a series of deployed models using the vetiver library\nIf there is additional time, I will attempt to create a shiny application that allows ease of access to the models and the data for stakeholders to explore but this may go out of scope as time permits.\n\n\n\n\n\nWho are on this project:\n\nStudent:\n\nKeenan Smith\n\nAdvisor:\n\nDr. Brandon McConnell\n\nFaculty Committee\n\n\n\n\n\n\nQualitative objectives\n\nAt least 4 deploy-able models using the Vetiver library that have been trained and optimized using grid search to find the highest accuracy model\nThe modeling and code must be left in a state where it is able to be redeployed with an expanded corpus\nThe hooks must be left so that there is potential for a generative model in the future\n\nWhat is a quantifiable metric (e.g. reduce the fraction of users with 4-week inactivity)\n\nAt least one model with a classification accuracy of 70% or more (This is subject to change as modeling is performed)\n\nWhat is the baseline (current) value of the metric?\n\nThe current baseline for this project is the null hypothesis that political speech does not indicate bias\n\nHow will we measure the metric? (e.g. A/B test on a specified subset for a specified period; or comparison of performance after implementation to baseline)\n\nThe metric will be measured by observing an accuracy score above 70%\n\n\n\n\n\n\nThis is a loose definition of the current plan\nBusiness Understanding\n\nDefining Objectives which is encapsulated in this project charter\nIdentifying Data Sources\nCurrently there are 4 but this will be expanded\nThe criteria is ease of the ability to scrape a large corpus of articles from the think tank, the explicit nature of the political bias present in popular culture, the ability to get the corpus into similar data as the currently acquired corpus\n\nData Acquisiton and Understanding\n\nIngest the Data into R and the model workflow\nWriting Data cleaning functions to clean the data prior to vectorization\nExplore the data using a variety of popular NLP vectorization techniques\n\nModeling\n\nFeature Engineering and Model Selection\nModeling the data using lessons learned from Exploration of the data\nEvaluating the Model\nAs a note, this project will utilize the tidymodels framework as much as possible as it uses a unified syntax across a variety of popular machine learning algorithms to output easily understood metrics and visualizations\n\nDeployment\n\nDeployed the selected models using the vetiver library.\nOptionally deploy the model using the plumber library to create an API for others to use the models\n\nThe key areas that I foresee that will take the most time is data acquisition and understanding. Using prior experience in this area and experience from others, data acquisition and cleaning the data take up to 70% of the work of a data analysis project. A project plan will be developed using project planning software to help plan this project in more detail for the stakeholders.\n\n\n\n\n\nData\n\nThe data will be provided via webscraping using the rvest package in R\nThe list of think tanks is currently being finalized at this current edition of the project charter\n\n\nWhat tools and data storage/analytics resources will be used in the solution e.g.,\n\nCurrently the data will be stored as CSV or .rda compressed data files\nSQLite will be explored as a way to store the data for deployment\n\nHow will the models be operationalized?\n\nThe models will be deployed using the vetiver library and possibly expanded to an API using the plumber library\n\n\n\n\n\n\nHow will we keep in touch? Weekly meetings?\n\nThere will be weekly update emails at a minimum to provide an update of current status. In-person or zoom meetings will be as required but at least occur on a bi-weekly basis."
  }
]